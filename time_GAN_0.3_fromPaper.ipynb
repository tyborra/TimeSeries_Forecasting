{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8cf16d7e",
   "metadata": {},
   "source": [
    "### Based on :\n",
    "- https://papers.nips.cc/paper/8789-time-series-generative-adversarial-networks\n",
    "    - https://github.com/jsyoon0823/TimeGAN\n",
    "    \n",
    "### Other\n",
    "- https://www.youtube.com/watch?v=ROLugVqjf00\n",
    "    - https://github.com/CasperHogenboom/WGAN_financial_time-series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e425319",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d0a6f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import GRU, Dense, RNN, GRUCell, Input\n",
    "from tensorflow.keras.losses import BinaryCrossentropy, MeanSquaredError\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "sns.set_style('whitegrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9a0d508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version: 2.6.0 /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "SEED = 4321\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "#Check GPU\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print('TF version:',tf.__version__ ,  tf.test.gpu_device_name() )\n",
    "\n",
    "#memory control \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "os.environ[\"TF_GPU_ALLOCATOR\"]=\"cuda_malloc_async\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2004f21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_built_with_cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a987aff",
   "metadata": {},
   "source": [
    "# Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2bfa092",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path = Path('time_gan')\n",
    "if not results_path.exists():\n",
    "    results_path.mkdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d948000c",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5cbf52f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = results_path / f'experiment_{experiment:02}'\n",
    "if not log_dir.exists():\n",
    "    log_dir.mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b83f25bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf_store = results_path / 'TimeSeriesGAN.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba68bd1",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3c4d568",
   "metadata": {},
   "outputs": [],
   "source": [
    "market1 = pd.read_csv('data/market1_train.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d28bdde8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-06-05</th>\n",
       "      <td>0.923487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-06</th>\n",
       "      <td>0.896356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-07</th>\n",
       "      <td>0.859538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-08</th>\n",
       "      <td>0.815306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-09</th>\n",
       "      <td>0.829723</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ID0\n",
       "2020-06-05  0.923487\n",
       "2020-06-06  0.896356\n",
       "2020-06-07  0.859538\n",
       "2020-06-08  0.815306\n",
       "2020-06-09  0.829723"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "market1 = pd.DataFrame(market1['ID0'])\n",
    "\n",
    "market1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8fc4d71e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "market1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fab19103",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(market1).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fca5b06",
   "metadata": {},
   "source": [
    "# Create Rolling WIndow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d11562c",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 50\n",
    "n_seq = 1    #columns\n",
    "batch_size = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "799b5558",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for i in range(len(market1) - seq_len):\n",
    "    data.append(scaled_data[i:i + seq_len])\n",
    "\n",
    "n_windows = len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0770ab27",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_series = (tf.data.Dataset\n",
    "               .from_tensor_slices(data)\n",
    "               .shuffle(buffer_size=n_windows)\n",
    "               .batch(batch_size))\n",
    "real_series_iter = iter(real_series.repeat())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322ece6c",
   "metadata": {},
   "source": [
    "# Random Series Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "78cc15e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_random_data():\n",
    "    while True:\n",
    "        yield np.random.uniform(low=0, high=1, size=(seq_len, n_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b5af7d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_series = iter(tf.data.Dataset\n",
    "                     .from_generator(make_random_data, output_types=tf.float32)\n",
    "                     .batch(batch_size)\n",
    "                     .repeat())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ac53d5",
   "metadata": {},
   "source": [
    "# TimeGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "35051577",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 24\n",
    "num_layers = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9454e5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = tf.summary.create_file_writer(log_dir.as_posix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7834d359",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = Input(shape=[seq_len, n_seq], name='RealData')\n",
    "Z = Input(shape=[seq_len, n_seq], name='RandomData')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "60710bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_rnn(n_layers, hidden_units, output_units, name):\n",
    "    return Sequential([GRU(units=hidden_units,\n",
    "                           return_sequences=True,\n",
    "                           name=f'GRU_{i + 1}') for i in range(n_layers)] +\n",
    "                      [Dense(units=output_units,\n",
    "                             activation='sigmoid',\n",
    "                             name='OUT')], name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "16d85961",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = make_rnn(n_layers=3, \n",
    "                    hidden_units=hidden_dim, \n",
    "                    output_units=hidden_dim, \n",
    "                    name='Embedder')\n",
    "recovery = make_rnn(n_layers=3, \n",
    "                    hidden_units=hidden_dim, \n",
    "                    output_units=n_seq, \n",
    "                    name='Recovery')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "46edb9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = make_rnn(n_layers=3, \n",
    "                     hidden_units=hidden_dim, \n",
    "                     output_units=hidden_dim, \n",
    "                     name='Generator')\n",
    "discriminator = make_rnn(n_layers=3, \n",
    "                         hidden_units=hidden_dim, \n",
    "                         output_units=1, \n",
    "                         name='Discriminator')\n",
    "supervisor = make_rnn(n_layers=2, \n",
    "                      hidden_units=hidden_dim, \n",
    "                      output_units=hidden_dim, \n",
    "                      name='Supervisor')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d57f442",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "08b9f24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_steps = 10000\n",
    "gamma = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "10ea5001",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = MeanSquaredError()\n",
    "bce = BinaryCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "709acd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "H = embedder(X)\n",
    "X_tilde = recovery(H)\n",
    "\n",
    "autoencoder = Model(inputs=X,\n",
    "                    outputs=X_tilde,\n",
    "                    name='Autoencoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "82e21ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Autoencoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "RealData (InputLayer)        [(None, 50, 1)]           0         \n",
      "_________________________________________________________________\n",
      "Embedder (Sequential)        (None, 50, 24)            9744      \n",
      "_________________________________________________________________\n",
      "Recovery (Sequential)        (None, 50, 1)             10825     \n",
      "=================================================================\n",
      "Total params: 20,569\n",
      "Trainable params: 20,569\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8768ddbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder_optimizer = Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ad06b411",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_autoencoder_init(x):\n",
    "    with tf.GradientTape() as tape:\n",
    "        x_tilde = autoencoder(x)\n",
    "        embedding_loss_t0 = mse(x, x_tilde)\n",
    "        e_loss_0 = 10 * tf.sqrt(embedding_loss_t0)\n",
    "\n",
    "    var_list = embedder.trainable_variables + recovery.trainable_variables\n",
    "    gradients = tape.gradient(e_loss_0, var_list)\n",
    "    autoencoder_optimizer.apply_gradients(zip(gradients, var_list))\n",
    "    return tf.sqrt(embedding_loss_t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dd76e41b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [08:00<00:00, 20.79it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for step in tqdm(range(train_steps)):\n",
    "    X_ = next(real_series_iter)\n",
    "    step_e_loss_t0 = train_autoencoder_init(X_)\n",
    "    with writer.as_default():\n",
    "        tf.summary.scalar('Loss Autoencoder Init', step_e_loss_t0, step=step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f65174",
   "metadata": {},
   "source": [
    "#https://www.tensorflow.org/tensorboard/tensorboard_profiling_keras\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be32131",
   "metadata": {},
   "source": [
    "%tensorboard --logdir=logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0877261",
   "metadata": {},
   "source": [
    "# Supervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b0607b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "supervisor_optimizer = Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2a0b36c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_supervisor(x):\n",
    "    with tf.GradientTape() as tape:\n",
    "        h = embedder(x)\n",
    "        h_hat_supervised = supervisor(h)\n",
    "        g_loss_s = mse(h[:, 1:, :], h_hat_supervised[:, 1:, :])\n",
    "\n",
    "    var_list = supervisor.trainable_variables\n",
    "    gradients = tape.gradient(g_loss_s, var_list)\n",
    "    supervisor_optimizer.apply_gradients(zip(gradients, var_list))\n",
    "    return g_loss_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "16837dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [09:03<00:00, 18.41it/s]\n"
     ]
    }
   ],
   "source": [
    "for step in tqdm(range(train_steps)):\n",
    "    X_ = next(real_series_iter)\n",
    "    step_g_loss_s = train_supervisor(X_)\n",
    "    with writer.as_default():\n",
    "        tf.summary.scalar('Loss Generator Supervised Init', step_g_loss_s, step=step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0ab392",
   "metadata": {},
   "source": [
    "# Adversarial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "eb6aa85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "E_hat = generator(Z)\n",
    "H_hat = supervisor(E_hat)\n",
    "Y_fake = discriminator(H_hat)\n",
    "\n",
    "adversarial_supervised = Model(inputs=Z,\n",
    "                               outputs=Y_fake,\n",
    "                               name='AdversarialNetSupervised')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3b8e7c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"AdversarialNetSupervised\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "RandomData (InputLayer)      [(None, 50, 1)]           0         \n",
      "_________________________________________________________________\n",
      "Generator (Sequential)       (None, 50, 24)            9744      \n",
      "_________________________________________________________________\n",
      "Supervisor (Sequential)      (None, 50, 24)            7800      \n",
      "_________________________________________________________________\n",
      "Discriminator (Sequential)   (None, 50, 1)             10825     \n",
      "=================================================================\n",
      "Total params: 28,369\n",
      "Trainable params: 28,369\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "adversarial_supervised.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c1a232eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) ', 'for plot_model/model_to_dot to work.')\n"
     ]
    }
   ],
   "source": [
    "plot_model(adversarial_supervised, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4b6d34a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"AdversarialNet\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "RandomData (InputLayer)      [(None, 50, 1)]           0         \n",
      "_________________________________________________________________\n",
      "Generator (Sequential)       (None, 50, 24)            9744      \n",
      "_________________________________________________________________\n",
      "Discriminator (Sequential)   (None, 50, 1)             10825     \n",
      "=================================================================\n",
      "Total params: 20,569\n",
      "Trainable params: 20,569\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "Y_fake_e = discriminator(E_hat)\n",
    "\n",
    "adversarial_emb = Model(inputs=Z,\n",
    "                    outputs=Y_fake_e,\n",
    "                    name='AdversarialNet')\n",
    "\n",
    "adversarial_emb.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "af6192ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_hat = recovery(H_hat)\n",
    "synthetic_data = Model(inputs=Z,\n",
    "                       outputs=X_hat,\n",
    "                       name='SyntheticData')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c504a5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_generator_moment_loss(y_true, y_pred):\n",
    "    y_true_mean, y_true_var = tf.nn.moments(x=y_true, axes=[0])\n",
    "    y_pred_mean, y_pred_var = tf.nn.moments(x=y_pred, axes=[0])\n",
    "    g_loss_mean = tf.reduce_mean(tf.abs(y_true_mean - y_pred_mean))\n",
    "    g_loss_var = tf.reduce_mean(tf.abs(tf.sqrt(y_true_var + 1e-6) - tf.sqrt(y_pred_var + 1e-6)))\n",
    "    return g_loss_mean + g_loss_var"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5243d00c",
   "metadata": {},
   "source": [
    "# Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a4e09bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_real = discriminator(H)\n",
    "discriminator_model = Model(inputs=X,\n",
    "                            outputs=Y_real,\n",
    "                            name='DiscriminatorReal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d6a81b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = Adam()\n",
    "discriminator_optimizer = Adam()\n",
    "embedding_optimizer = Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "65054695",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_generator(x, z):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_fake = adversarial_supervised(z)\n",
    "        generator_loss_unsupervised = bce(y_true=tf.ones_like(y_fake),\n",
    "                                          y_pred=y_fake)\n",
    "\n",
    "        y_fake_e = adversarial_emb(z)\n",
    "        generator_loss_unsupervised_e = bce(y_true=tf.ones_like(y_fake_e),\n",
    "                                            y_pred=y_fake_e)\n",
    "        h = embedder(x)\n",
    "        h_hat_supervised = supervisor(h)\n",
    "        generator_loss_supervised = mse(h[:, 1:, :], h_hat_supervised[:, 1:, :])\n",
    "\n",
    "        x_hat = synthetic_data(z)\n",
    "        generator_moment_loss = get_generator_moment_loss(x, x_hat)\n",
    "\n",
    "        generator_loss = (generator_loss_unsupervised +\n",
    "                          generator_loss_unsupervised_e +\n",
    "                          100 * tf.sqrt(generator_loss_supervised) +\n",
    "                          100 * generator_moment_loss)\n",
    "\n",
    "    var_list = generator.trainable_variables + supervisor.trainable_variables\n",
    "    gradients = tape.gradient(generator_loss, var_list)\n",
    "    generator_optimizer.apply_gradients(zip(gradients, var_list))\n",
    "    return generator_loss_unsupervised, generator_loss_supervised, generator_moment_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "19794772",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_embedder(x):\n",
    "    with tf.GradientTape() as tape:\n",
    "        h = embedder(x)\n",
    "        h_hat_supervised = supervisor(h)\n",
    "        generator_loss_supervised = mse(h[:, 1:, :], h_hat_supervised[:, 1:, :])\n",
    "\n",
    "        x_tilde = autoencoder(x)\n",
    "        embedding_loss_t0 = mse(x, x_tilde)\n",
    "        e_loss = 10 * tf.sqrt(embedding_loss_t0) + 0.1 * generator_loss_supervised\n",
    "\n",
    "    var_list = embedder.trainable_variables + recovery.trainable_variables\n",
    "    gradients = tape.gradient(e_loss, var_list)\n",
    "    embedding_optimizer.apply_gradients(zip(gradients, var_list))\n",
    "    return tf.sqrt(embedding_loss_t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "faa4c3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def get_discriminator_loss(x, z):\n",
    "    y_real = discriminator_model(x)\n",
    "    discriminator_loss_real = bce(y_true=tf.ones_like(y_real),\n",
    "                                  y_pred=y_real)\n",
    "\n",
    "    y_fake = adversarial_supervised(z)\n",
    "    discriminator_loss_fake = bce(y_true=tf.zeros_like(y_fake),\n",
    "                                  y_pred=y_fake)\n",
    "\n",
    "    y_fake_e = adversarial_emb(z)\n",
    "    discriminator_loss_fake_e = bce(y_true=tf.zeros_like(y_fake_e),\n",
    "                                    y_pred=y_fake_e)\n",
    "    return (discriminator_loss_real +\n",
    "            discriminator_loss_fake +\n",
    "            gamma * discriminator_loss_fake_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f5b21970",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_discriminator(x, z):\n",
    "    with tf.GradientTape() as tape:\n",
    "        discriminator_loss = get_discriminator_loss(x, z)\n",
    "\n",
    "    var_list = discriminator.trainable_variables\n",
    "    gradients = tape.gradient(discriminator_loss, var_list)\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients, var_list))\n",
    "    return discriminator_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99097001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0 | d_loss: 2.2003 | g_loss_u: 0.6084 | g_loss_s: 0.0003 | g_loss_v: 0.3510 | e_loss_t0: 0.0649\n"
     ]
    }
   ],
   "source": [
    "step_g_loss_u = step_g_loss_s = step_g_loss_v = step_e_loss_t0 = step_d_loss = 0\n",
    "for step in range(train_steps):\n",
    "    # Train generator (twice as often as discriminator)\n",
    "    for kk in range(2):\n",
    "        X_ = next(real_series_iter)\n",
    "        Z_ = next(random_series)\n",
    "\n",
    "        # Train generator\n",
    "        step_g_loss_u, step_g_loss_s, step_g_loss_v = train_generator(X_, Z_)\n",
    "        # Train embedder\n",
    "        step_e_loss_t0 = train_embedder(X_)\n",
    "\n",
    "    X_ = next(real_series_iter)\n",
    "    Z_ = next(random_series)\n",
    "    step_d_loss = get_discriminator_loss(X_, Z_)\n",
    "    if step_d_loss > 0.15:\n",
    "        step_d_loss = train_discriminator(X_, Z_)\n",
    "\n",
    "    if step % 1000 == 0:\n",
    "        print(f'{step:6,.0f} | d_loss: {step_d_loss:6.4f} | g_loss_u: {step_g_loss_u:6.4f} | '\n",
    "              f'g_loss_s: {step_g_loss_s:6.4f} | g_loss_v: {step_g_loss_v:6.4f} | e_loss_t0: {step_e_loss_t0:6.4f}')\n",
    "\n",
    "    with writer.as_default():\n",
    "        tf.summary.scalar('G Loss S', step_g_loss_s, step=step)\n",
    "        tf.summary.scalar('G Loss U', step_g_loss_u, step=step)\n",
    "        tf.summary.scalar('G Loss V', step_g_loss_v, step=step)\n",
    "        tf.summary.scalar('E Loss T0', step_e_loss_t0, step=step)\n",
    "        tf.summary.scalar('D Loss', step_d_loss, step=step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a924a185",
   "metadata": {},
   "source": [
    "# Synthetic Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561a0281",
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_data.save(log_dir / 'synthetic_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2108273",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_data = []\n",
    "for i in range(int(n_windows / batch_size)):\n",
    "    Z_ = next(random_series)\n",
    "    d = synthetic_data(Z_)\n",
    "    generated_data.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a5c1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(generated_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98849f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_data = np.array(np.vstack(generated_data))\n",
    "generated_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ef9831",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(log_dir / 'generated_data.npy', generated_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096a8f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_data = (scaler.inverse_transform(generated_data\n",
    "                                           .reshape(-1, n_seq))\n",
    "                  .reshape(-1, seq_len, n_seq))\n",
    "generated_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbddfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.HDFStore(hdf_store) as store:\n",
    "    store.put('data/synthetic', pd.DataFrame(generated_data.reshape(-1, n_seq),\n",
    "                                             columns=tickers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09037cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(14, 7))\n",
    "axes = axes.flatten()\n",
    "\n",
    "index = list(range(1, 25))\n",
    "synthetic = generated_data[np.random.randint(n_windows)]\n",
    "\n",
    "idx = np.random.randint(len(market1) - seq_len)\n",
    "real = market1.iloc[idx: idx + seq_len]\n",
    "\n",
    "tickers = market1.columns[:6]\n",
    "\n",
    "for j, ticker in enumerate(tickers):\n",
    "    (pd.DataFrame({'Real': real.iloc[:, j].values,\n",
    "                   'Synthetic': synthetic[:, j]})\n",
    "     .plot(ax=axes[j],\n",
    "           title=ticker,\n",
    "           secondary_y='Synthetic', style=['-', '--'],\n",
    "           lw=1))\n",
    "sns.despine()\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b94040a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
